{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOBfCDrjKFN2vXiRS6Ir7nV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4227-cyber/Bird_Golden/blob/main/BirdG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XMBfuyE9RT0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85811138-eb1c-41d1-947c-5061670d46d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mask-RCNN_TF2.14.0'...\n",
            "remote: Enumerating objects: 246, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 246 (delta 27), reused 15 (delta 15), pack-reused 192 (from 1)\u001b[K\n",
            "Receiving objects: 100% (246/246), 74.90 MiB | 34.03 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/z-mahmud22/Mask-RCNN_TF2.14.0.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Mask-RCNN_TF2.14.0\n"
      ],
      "metadata": {
        "id": "ViWrJv0MRZyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80ab456-daa9-47f9-84f7-27fb85fe184c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask-RCNN_TF2.14.0/Mask-RCNN_TF2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)\n"
      ],
      "metadata": {
        "id": "sRIjyomGRevH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b25928-4b8d-4a97-c96d-1a27ae6993e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o requirements.txt https://raw.githubusercontent.com/z-mahmud22/Mask-RCNN_TF2.14.0/main/requirements.txt\n"
      ],
      "metadata": {
        "id": "J1FK8hrkRhw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c470d76d-f132-4695-d6e3-77520d214d2c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   299  100   299    0     0   1201      0 --:--:-- --:--:-- --:--:--  1205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "PM4wuQ4lRj9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d22fe469-83aa-4d48-f776-70e99049baf2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cython==3.0.5 (from -r requirements.txt (line 1))\n",
            "  Downloading Cython-3.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting h5py==3.9.0 (from -r requirements.txt (line 2))\n",
            "  Downloading h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: imgaug==0.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (7.34.0)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: ipython-sql==0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.5.0)\n",
            "Collecting keras==2.14.0 (from -r requirements.txt (line 7))\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting matplotlib==3.7.1 (from -r requirements.txt (line 8))\n",
            "  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting numpy==1.23.5 (from -r requirements.txt (line 9))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting opencv-contrib-python==4.8.0.76 (from -r requirements.txt (line 10))\n",
            "  Downloading opencv_contrib_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting opencv-python==4.8.0.76 (from -r requirements.txt (line 11))\n",
            "  Downloading opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting pillow==9.4.0 (from -r requirements.txt (line 12))\n",
            "  Downloading Pillow-9.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting scikit-image==0.19.3 (from -r requirements.txt (line 13))\n",
            "  Downloading scikit_image-0.19.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Collecting scipy==1.11.3 (from -r requirements.txt (line 14))\n",
            "  Downloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard==2.14.1 (from -r requirements.txt (line 15))\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow==2.14.0 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug==0.4.0->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug==0.4.0->-r requirements.txt (line 3)) (2.36.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug==0.4.0->-r requirements.txt (line 3)) (2.0.6)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython==7.34.0->-r requirements.txt (line 4))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (4.9.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from ipython-sql==0.5.0->-r requirements.txt (line 6)) (3.12.0)\n",
            "Requirement already satisfied: sqlalchemy>=2.0 in /usr/local/lib/python3.10/dist-packages (from ipython-sql==0.5.0->-r requirements.txt (line 6)) (2.0.36)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.10/dist-packages (from ipython-sql==0.5.0->-r requirements.txt (line 6)) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 13)) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 13)) (2024.12.12)\n",
            "Collecting PyWavelets>=1.1.1 (from scikit-image==0.19.3->-r requirements.txt (line 13))\n",
            "  Downloading pywavelets-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (1.68.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard==2.14.1->-r requirements.txt (line 15))\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (3.7)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (3.1.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (18.1.1)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (3.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (0.37.1)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.8.89 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cublas-cu11==11.11.3.6 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.7.0.84 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-curand-cu11==10.3.0.86 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_curand_cu11-10.3.0.86-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.1.48 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.5.86 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-nccl-cu11==2.16.5 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_nccl_cu11-2.16.5-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.8.87 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvcc-cu11==11.8.89 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cuda_nvcc_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting tensorrt==8.5.3.1 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading tensorrt-8.5.3.1-cp310-none-manylinux_2_17_x86_64.whl.metadata (721 bytes)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (0.45.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.14.1->-r requirements.txt (line 15)) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.14.1->-r requirements.txt (line 15)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.14.1->-r requirements.txt (line 15)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard==2.14.1->-r requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==7.34.0->-r requirements.txt (line 4)) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==7.34.0->-r requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->-r requirements.txt (line 4)) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.1->-r requirements.txt (line 15)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.1->-r requirements.txt (line 15)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.1->-r requirements.txt (line 15)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.1->-r requirements.txt (line 15)) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0->ipython-sql==0.5.0->-r requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.14.1->-r requirements.txt (line 15)) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.14.1->-r requirements.txt (line 15)) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard==2.14.1->-r requirements.txt (line 15)) (3.2.2)\n",
            "Downloading Cython-3.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Pillow-9.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_image-0.19.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl (417.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvcc_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl (875 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.3.0.86-py3-none-manylinux2014_x86_64.whl (58.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux2014_x86_64.whl (128.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux2014_x86_64.whl (204.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.16.5-py3-none-manylinux1_x86_64.whl (210.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.3/210.3 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorrt-8.5.3.1-cp310-none-manylinux_2_17_x86_64.whl (549.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.5/549.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, pillow, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvcc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, keras, jedi, cython, scipy, PyWavelets, opencv-python, opencv-contrib-python, nvidia-cusolver-cu11, nvidia-cudnn-cu11, ml-dtypes, h5py, tensorrt, scikit-image, matplotlib, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.0\n",
            "    Uninstalling wrapt-1.17.0:\n",
            "      Successfully uninstalled wrapt-1.17.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.0.0\n",
            "    Uninstalling pillow-11.0.0:\n",
            "      Successfully uninstalled pillow-11.0.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.11\n",
            "    Uninstalling Cython-3.0.11:\n",
            "      Successfully uninstalled Cython-3.0.11\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.10.0.84\n",
            "    Uninstalling opencv-python-4.10.0.84:\n",
            "      Successfully uninstalled opencv-python-4.10.0.84\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.10.0.84\n",
            "    Uninstalling opencv-contrib-python-4.10.0.84:\n",
            "      Successfully uninstalled opencv-contrib-python-4.10.0.84\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.12.1\n",
            "    Uninstalling h5py-3.12.1:\n",
            "      Successfully uninstalled h5py-3.12.1\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.25.0\n",
            "    Uninstalling scikit-image-0.25.0:\n",
            "      Successfully uninstalled scikit-image-0.25.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.0\n",
            "    Uninstalling matplotlib-3.8.0:\n",
            "      Successfully uninstalled matplotlib-3.8.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.29.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "google-genai 0.3.0 requires pillow<12.0.0,>=10.0.0, but you have pillow 9.4.0 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "plotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n",
            "pymc 5.19.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorstore 0.1.71 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.14.0 which is incompatible.\n",
            "xarray 2024.11.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyWavelets-1.8.0 cython-3.0.5 google-auth-oauthlib-1.0.0 h5py-3.9.0 jedi-0.19.2 keras-2.14.0 matplotlib-3.7.1 ml-dtypes-0.2.0 numpy-1.23.5 nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvcc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.16.5 opencv-contrib-python-4.8.0.76 opencv-python-4.8.0.76 pillow-9.4.0 scikit-image-0.19.3 scipy-1.11.3 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorrt-8.5.3.1 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              },
              "id": "4ad3c9447be3440ca1d2dfc9ea4b0637"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 setup.py build\n",
        "!python3 setup.py install\n"
      ],
      "metadata": {
        "id": "fYmbK2GIRlrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a9c6e2-698c-43bc-ea25-1a6fb8ee6548"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask-RCNN_TF2.14.0/Mask-RCNN_TF2.14.0/setup.py:9: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n",
            "WARNING:root:Fail load requirements file, so using default ones.\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'description-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'description_file' instead.\n",
            "\n",
            "        This deprecation is overdue, please update your project and remove deprecated\n",
            "        calls to avoid build errors in the future.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'license-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'license_file' instead.\n",
            "\n",
            "        This deprecation is overdue, please update your project and remove deprecated\n",
            "        calls to avoid build errors in the future.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'requirements-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'requirements_file' instead.\n",
            "\n",
            "        This deprecation is overdue, please update your project and remove deprecated\n",
            "        calls to avoid build errors in the future.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "INFO:root:running build\n",
            "INFO:root:running build_py\n",
            "INFO:root:creating build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/__init__.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/utils.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/parallel_model.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/model.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/config.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/visualize.py -> build/lib/mrcnn\n",
            "INFO:root:running egg_info\n",
            "INFO:root:creating mask_rcnn_tf2.egg-info\n",
            "INFO:root:writing mask_rcnn_tf2.egg-info/PKG-INFO\n",
            "INFO:root:writing dependency_links to mask_rcnn_tf2.egg-info/dependency_links.txt\n",
            "INFO:root:writing top-level names to mask_rcnn_tf2.egg-info/top_level.txt\n",
            "INFO:root:writing manifest file 'mask_rcnn_tf2.egg-info/SOURCES.txt'\n",
            "INFO:root:reading manifest file 'mask_rcnn_tf2.egg-info/SOURCES.txt'\n",
            "INFO:root:reading manifest template 'MANIFEST.in'\n",
            "INFO:root:adding license file 'LICENSE'\n",
            "INFO:root:writing manifest file 'mask_rcnn_tf2.egg-info/SOURCES.txt'\n",
            "/content/Mask-RCNN_TF2.14.0/Mask-RCNN_TF2.14.0/setup.py:9: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n",
            "WARNING:root:Fail load requirements file, so using default ones.\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'description-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'description_file' instead.\n",
            "\n",
            "        This deprecation is overdue, please update your project and remove deprecated\n",
            "        calls to avoid build errors in the future.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'license-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'license_file' instead.\n",
            "\n",
            "        This deprecation is overdue, please update your project and remove deprecated\n",
            "        calls to avoid build errors in the future.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'requirements-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'requirements_file' instead.\n",
            "\n",
            "        This deprecation is overdue, please update your project and remove deprecated\n",
            "        calls to avoid build errors in the future.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "INFO:root:running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "INFO:root:running bdist_egg\n",
            "INFO:root:running egg_info\n",
            "INFO:root:writing mask_rcnn_tf2.egg-info/PKG-INFO\n",
            "INFO:root:writing dependency_links to mask_rcnn_tf2.egg-info/dependency_links.txt\n",
            "INFO:root:writing top-level names to mask_rcnn_tf2.egg-info/top_level.txt\n",
            "INFO:root:reading manifest file 'mask_rcnn_tf2.egg-info/SOURCES.txt'\n",
            "INFO:root:reading manifest template 'MANIFEST.in'\n",
            "INFO:root:adding license file 'LICENSE'\n",
            "INFO:root:writing manifest file 'mask_rcnn_tf2.egg-info/SOURCES.txt'\n",
            "INFO:root:installing library code to build/bdist.linux-x86_64/egg\n",
            "INFO:root:running install_lib\n",
            "INFO:root:running build_py\n",
            "INFO:root:copying mrcnn/__init__.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/utils.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/parallel_model.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/model.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/config.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/visualize.py -> build/lib/mrcnn\n",
            "INFO:root:creating build/bdist.linux-x86_64/egg\n",
            "INFO:root:creating build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-310.pyc\n",
            "INFO:root:byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-310.pyc\n",
            "INFO:root:byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-310.pyc\n",
            "INFO:root:byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-310.pyc\n",
            "INFO:root:byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-310.pyc\n",
            "INFO:root:byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-310.pyc\n",
            "INFO:root:creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "INFO:root:copying mask_rcnn_tf2.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "INFO:root:copying mask_rcnn_tf2.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "INFO:root:copying mask_rcnn_tf2.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "INFO:root:copying mask_rcnn_tf2.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "WARNING:root:zip_safe flag not set; analyzing archive contents...\n",
            "INFO:root:creating dist\n",
            "INFO:root:creating 'dist/mask_rcnn_tf2-1.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "INFO:root:removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "INFO:root:Processing mask_rcnn_tf2-1.0-py3.10.egg\n",
            "INFO:root:Copying mask_rcnn_tf2-1.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "INFO:root:Adding mask-rcnn-tf2 1.0 to easy-install.pth file\n",
            "INFO:root:\n",
            "Installed /usr/local/lib/python3.10/dist-packages/mask_rcnn_tf2-1.0-py3.10.egg\n",
            "INFO:root:Processing dependencies for mask-rcnn-tf2==1.0\n",
            "INFO:root:Finished processing dependencies for mask-rcnn-tf2==1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/matterport/Mask_RCNN\n"
      ],
      "metadata": {
        "id": "h98Rx7mnSaOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe0bbf3-f5af-4689-e8dd-21e302e0d617"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-22 17:02:21--  https://github.com/matterport/Mask_RCNN\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘Mask_RCNN’\n",
            "\n",
            "Mask_RCNN               [ <=>                ] 373.56K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-12-22 17:02:21 (10.6 MB/s) - ‘Mask_RCNN’ saved [382526]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "UTXA-sEPScvD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0de147f-cb61-4162-a6c7-a2d4ab60ff0d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# Specify the path to your dataset (change the path accordingly)\n",
        "data_dir = '/content/drive/MyDrive/Bird'"
      ],
      "metadata": {
        "id": "2OBb_m2zSewf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "from mrcnn.utils import Dataset\n",
        "\n",
        "class BirdDataset(Dataset):\n",
        "    def load_bird(self, dataset_dir, subset):\n",
        "        \"\"\"\n",
        "       Load a subset of the Bird dataset (train, val, test).\n",
        "        dataset_dir: Root directory of the dataset.\n",
        "        subset: The subset to load: 'train', 'val', or 'test'.\n",
        "        \"\"\"\n",
        "        # Ajouter la classe \"golden_pheasant\" (ou toute autre classe appropriée)\n",
        "        self.add_class(\"bird\", 1, \"golden_pheasant\")  # Vous pouvez ajouter d'autres classes ici si nécessaire\n",
        "\n",
        "        # Définir le chemin du fichier d'annotations pour le sous-ensemble actuel\n",
        "        annotations_path = os.path.join(dataset_dir, subset, \"annotations.coco.json\")\n",
        "        print(annotations_path)\n",
        "\n",
        "        # Charger les annotations COCO pour le sous-ensemble actuel\n",
        "        coco = COCO(annotations_path)\n",
        "\n",
        "        # Ajouter les images au dataset\n",
        "        image_ids = coco.getImgIds()\n",
        "        for image_id in image_ids:\n",
        "            image_info = coco.loadImgs(image_id)[0]\n",
        "            image_path = os.path.join(dataset_dir, subset, image_info['file_name'])\n",
        "\n",
        "            # Vérifier si l'image existe avant de l'ajouter\n",
        "            if os.path.exists(image_path):\n",
        "                self.add_image(\n",
        "                    \"bird\",  # Nom de la catégorie\n",
        "                    image_id=image_id,\n",
        "                    path=image_path,\n",
        "                    #width=image_info['width'],\n",
        "                    #height=image_info['height']\n",
        "                )\n",
        "            else:\n",
        "                print(f\"Image not found: {image_path}\")\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "\n",
        "\n",
        "        info = self.image_info[image_id]\n",
        "        # Charger les annotations correspondantes pour l'image donnée\n",
        "        annotations_path = os.path.join(os.path.dirname(info['path']), 'annotations.coco.json')\n",
        "        coco = COCO(annotations_path)\n",
        "        ann_ids = coco.getAnnIds(imgIds=info['id'])\n",
        "        annotations = coco.loadAnns(ann_ids)\n",
        "\n",
        "        masks = []\n",
        "        class_ids = []\n",
        "\n",
        "        # Utiliser les boîtes englobantes (bounding boxes) pour créer des masques\n",
        "        for ann in annotations:\n",
        "            # Vérifier si l'annotation contient une boîte englobante\n",
        "            if 'bbox' in ann:\n",
        "                bbox = ann['bbox']\n",
        "                mask = np.zeros((info['height'], info['width']), dtype=np.uint8)\n",
        "\n",
        "                # L'annotation bbox est sous la forme [x, y, largeur, hauteur]\n",
        "                x, y, width, height = bbox\n",
        "                mask[y:y+height, x:x+width] = 1  # Marquer la zone de la boîte avec 1\n",
        "\n",
        "                masks.append(mask)\n",
        "                class_ids.append(ann['category_id'])  # Ajouter l'ID de la catégorie\n",
        "\n",
        "        # Si aucune annotation n'est présente (boîte englobante vide), renvoyer un masque vide\n",
        "        if len(masks) == 0:\n",
        "            masks = np.zeros((info['height'], info['width'], 0))  # Masque vide\n",
        "            class_ids = np.array([])  # Pas de classes\n",
        "\n",
        "        # Empiler les masques et convertir class_ids en tableau numpy\n",
        "        return np.stack(masks, axis=-1), np.array(class_ids)\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "\n",
        "\n",
        "        return self.image_info[image_id]['path']"
      ],
      "metadata": {
        "id": "k4nX4qTOJ6M5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir les chemins vers les données\n",
        "dataset_dir = '/content/drive/MyDrive/Bird/'  # Répertoire principal contenant les sous-dossiers train et test\n",
        "subset = 'train'\n",
        "# Initialiser les datasets\n",
        "dataset_train = BirdDataset()\n",
        "\n",
        "\n",
        "# Charger les données de l'ensemble d'entraînement\n",
        "dataset_train.load_bird(dataset_dir, \"train\")  # Assurez-vous que \"train\" est un répertoire valide\n",
        "dataset_train.prepare()\n"
      ],
      "metadata": {
        "id": "PDwO3i9QVV6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e56ca5-7d27-4776-c0fd-b5ead42fe930"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Bird/train/annotations.coco.json\n",
            "loading annotations into memory...\n",
            "Done (t=1.08s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir les chemins vers les données\n",
        "dataset_dir = '/content/drive/MyDrive/Bird/'  # Répertoire principal contenant les sous-dossiers train et val\n",
        "subset = 'val'  # Sous-ensemble 'val' pour la validation\n",
        "\n",
        "# Initialiser les datasets\n",
        "dataset_val = BirdDataset()\n",
        "\n",
        "# Charger les données de l'ensemble de validation, y compris les annotations depuis le fichier\n",
        "dataset_val.load_bird(dataset_dir, subset=subset)  # Passer le fichier d'annotations\n",
        "dataset_val.prepare()  # Préparer l'ensemble de données pour la validation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNukjo52rvZG",
        "outputId": "de53a08f-43af-4226-8585-18adde3a32c1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Bird/val/annotations.coco.json\n",
            "loading annotations into memory...\n",
            "Done (t=0.57s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir les chemins\n",
        "dataset_dir = '/content/drive/MyDrive/Bird/'\n",
        "subset = 'test'\n",
        "\n",
        "# Initialiser le dataset\n",
        "dataset_test = BirdDataset()\n",
        "\n",
        "# Charger le dataset à partir du fichier d'annotations\n",
        "dataset_test.load_bird(dataset_dir, subset=subset)  # Charger les données\n",
        "dataset_test.prepare()  # Préparer le dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M3pIDlcysHt",
        "outputId": "e8ffd26c-c492-475b-b06a-5f9c68c356d0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Bird/test/annotations.coco.json\n",
            "loading annotations into memory...\n",
            "Done (t=0.49s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.model import MaskRCNN\n",
        "\n",
        "# Define your custom configuration class\n",
        "class CustomConfig(Config):\n",
        "    NAME = \"Bird\"  # Name of the dataset or task\n",
        "    IMAGES_PER_GPU = 2  # Adjust this based on the memory of your GPU\n",
        "    NUM_CLASSES = 2  # Including background (1) + Bird (1)\n",
        "    STEPS_PER_EPOCH = 100  # Set based on the number of images in your dataset\n",
        "    VALIDATION_STEPS = 100  # Set validation steps based on the dataset size\n",
        "    LEARNING_RATE = 0.001  # Learning rate for training\n",
        "    IMAGE_MIN_DIM = 512  # Minimum image dimension for resizing\n",
        "    IMAGE_MAX_DIM = 512  # Maximum image dimension for resizing\n",
        "    GPU_COUNT = 1  # Number of GPUs (adjust if you have more)\n",
        "\n",
        "# Instantiate your custom configuration\n",
        "config = CustomConfig()"
      ],
      "metadata": {
        "id": "ZbJFwnA1U9yi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77V4u-GyGFxH",
        "outputId": "1ad3b18a-d24a-4af9-92be-6ed5bef17ff9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-22 17:02:47--  https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241222%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241222T170247Z&X-Amz-Expires=300&X-Amz-Signature=4313b58fddb0b44dcb78b715f953c9cb48abb7b56c33ad66591ff51db6adc6d3&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-22 17:02:48--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241222%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241222T170247Z&X-Amz-Expires=300&X-Amz-Signature=4313b58fddb0b44dcb78b715f953c9cb48abb7b56c33ad66591ff51db6adc6d3&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257557808 (246M) [application/octet-stream]\n",
            "Saving to: ‘mask_rcnn_coco.h5’\n",
            "\n",
            "mask_rcnn_coco.h5   100%[===================>] 245.63M   103MB/s    in 2.4s    \n",
            "\n",
            "2024-12-22 17:02:50 (103 MB/s) - ‘mask_rcnn_coco.h5’ saved [257557808/257557808]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create the model in training mode\n",
        "model = MaskRCNN(mode=\"training\", config=config, model_dir='/content')\n",
        "\n",
        "# Step 2: Load pre-trained weights (COCO weights) but exclude the conflicting layers\n",
        "model.load_weights('mask_rcnn_coco.h5', by_name=True, exclude=[\"mrcnn_bbox_fc\", \"mrcnn_class_logits\", \"mrcnn_mask\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "1F186C_AVCZR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(dataset_train, dataset_val, learning_rate=config.LEARNING_RATE, epochs=10, layers='heads')\n"
      ],
      "metadata": {
        "id": "8WapeIVjwlSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e429c5dd-0089-4783-d824-5d0f8e8bbe7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting at epoch 0. LR=0.001\n",
            "\n",
            "Checkpoint Path: /content/bird20241222T1703/mask_rcnn_bird_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "rpn_model              (Functional)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Generate Confusion Matrix after training\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Step 4.1: Create a model for inference\n",
        "class InferenceConfig(Config):\n",
        "    NAME = \"bird\"\n",
        "    NUM_CLASSES = 1 + 1\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "inference_model = MaskRCNN(mode=\"inference\", config=inference_config, model_dir='/content/drive/Mydrive/Bird/data/train')\n",
        "\n",
        "# Load the trained weights into the inference model\n",
        "inference_model.load_weights(model.find_last(), by_name=True)\n",
        "\n",
        "# Step 4.2: Collect true and predicted labels\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for image_id in dataset_test.image_ids:\n",
        "    # Load true labels (class IDs) from the dataset\n",
        "    true_masks, true_classes = dataset_test.load_mask(image_id)\n",
        "\n",
        "    # Predict the image using the inference model\n",
        "    image = dataset_test.load_image(image_id)\n",
        "    pred = inference_model.detect([image], verbose=0)[0]\n",
        "\n",
        "    # Align true and predicted classes\n",
        "    if len(true_classes) == len(pred['class_ids']):\n",
        "        true_labels.extend(true_classes)\n",
        "        pred_labels.extend(pred['class_ids'])\n",
        "    else:\n",
        "        # Handle mismatched cases\n",
        "        true_labels.extend(true_classes)\n",
        "        pred_labels.extend(pred['class_ids'])\n",
        "\n",
        "        # Fill with '0' (background class) for unmatched entries\n",
        "        if len(true_classes) > len(pred['class_ids']):\n",
        "            pred_labels.extend([0] * (len(true_classes) - len(pred['class_ids'])))\n",
        "        elif len(pred['class_ids']) > len(true_classes):\n",
        "            true_labels.extend([0] * (len(pred['class_ids']) - len(true_classes)))\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 4.3: Generate and plot the confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1])  # Add '0' for background\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['background', 'Bird_Golden'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Step 5: Visualize test predictions (Successes and Mistakes)\n",
        "from mrcnn.visualize import display_instances\n",
        "from mrcnn import visualize\n",
        "\n",
        "import random\n",
        "\n",
        "# Select 5 random image IDs from the test dataset\n",
        "for i in random.sample(list(dataset_test.image_ids), 5):\n",
        "    image = dataset_test.load_image(i)  # Load the image\n",
        "    results = inference_model.detect([image], verbose=0)  # Perform detection\n",
        "\n",
        "    # Visualize the prediction\n",
        "    visualize.display_instances(\n",
        "        image,\n",
        "        results[0]['rois'],\n",
        "        results[0]['masks'],\n",
        "        results[0]['class_ids'],\n",
        "        dataset_test.class_names,\n",
        "        results[0]['scores']\n",
        "    )"
      ],
      "metadata": {
        "id": "oOYnssGXVEbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mrcnn.utils import compute_ap\n",
        "import numpy as np\n",
        "from mrcnn import utils\n",
        "\n",
        "# Store Average Precision (AP) for each image\n",
        "APs = []\n",
        "\n",
        "# Iterate over all test images\n",
        "for image_id in dataset_test.image_ids:\n",
        "    # Load the image\n",
        "    image = dataset_test.load_image(image_id)\n",
        "\n",
        "    # Load ground truth data: masks and class IDs\n",
        "    gt_mask, gt_class_ids = dataset_test.load_mask(image_id)\n",
        "\n",
        "    # Generate ground truth bounding boxes (compute from masks if not pre-defined)\n",
        "    gt_bbox = utils.extract_bboxes(gt_mask)\n",
        "\n",
        "    # Run inference on the image\n",
        "    results = inference_model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "\n",
        "    # Skip images with no predictions\n",
        "    if r['masks'].size == 0:\n",
        "        continue\n",
        "\n",
        "    # Ensure predicted masks are the same size as the ground truth masks\n",
        "    pred_masks = r['masks']\n",
        "\n",
        "    # If the prediction masks are larger than needed, resize them to match ground truth\n",
        "    if pred_masks.shape[0] != gt_mask.shape[0] or pred_masks.shape[1] != gt_mask.shape[1]:\n",
        "        pred_masks_resized = []\n",
        "        for i in range(pred_masks.shape[-1]):\n",
        "            mask = pred_masks[:, :, i]\n",
        "            mask_resized = cv2.resize(mask.astype(np.uint8),\n",
        "                                    (gt_mask.shape[1], gt_mask.shape[0]),\n",
        "                                    interpolation=cv2.INTER_NEAREST)\n",
        "            pred_masks_resized.append(mask_resized)\n",
        "        pred_masks = np.stack(pred_masks_resized, axis=-1)\n",
        "\n",
        "    # Skip if there are no valid ground truth or predictions\n",
        "    if gt_class_ids.size == 0 or r['class_ids'].size == 0:\n",
        "        continue\n",
        "\n",
        "    # Compute AP for the current image\n",
        "    AP, precisions, recalls, overlaps = compute_ap(\n",
        "        gt_bbox,          # Ground truth bounding boxes\n",
        "        gt_class_ids,     # Ground truth class IDs\n",
        "        gt_mask,          # Ground truth masks\n",
        "        r['rois'],        # Predicted bounding boxes\n",
        "        r['class_ids'],   # Predicted class IDs\n",
        "        r['scores'],      # Predicted scores (was missing in original)\n",
        "        pred_masks        # Predicted masks\n",
        "    )\n",
        "\n",
        "    APs.append(AP)\n",
        "\n",
        "# Compute and display the mean Average Precision (mAP)\n",
        "if len(APs) > 0:\n",
        "    mAP = np.mean(APs)\n",
        "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
        "else:\n",
        "    print(\"No valid predictions found for evaluation\")"
      ],
      "metadata": {
        "id": "lf3wOQQDVPhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mrcnn.utils import compute_ap\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import cv2\n",
        "\n",
        "# Store metrics for each image\n",
        "APs = []\n",
        "all_true_labels = []\n",
        "all_pred_labels = []\n",
        "\n",
        "# Iterate over all test images\n",
        "for image_id in dataset_test.image_ids:\n",
        "    # Load the image\n",
        "    image = dataset_test.load_image(image_id)\n",
        "\n",
        "    # Load ground truth data: masks and class IDs\n",
        "    gt_mask, gt_class_ids = dataset_test.load_mask(image_id)\n",
        "\n",
        "    # Generate ground truth bounding boxes\n",
        "    gt_bbox = utils.extract_bboxes(gt_mask)\n",
        "\n",
        "    # Run inference on the image\n",
        "    results = inference_model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "\n",
        "    # Skip images with no predictions\n",
        "    if r['masks'].size == 0:\n",
        "        continue\n",
        "\n",
        "    # Ensure predicted masks are the same size as the ground truth masks\n",
        "    pred_masks = r['masks']\n",
        "\n",
        "    # Resize prediction masks if needed\n",
        "    if pred_masks.shape[0] != gt_mask.shape[0] or pred_masks.shape[1] != gt_mask.shape[1]:\n",
        "        pred_masks_resized = []\n",
        "        for i in range(pred_masks.shape[-1]):\n",
        "            mask = pred_masks[:, :, i]\n",
        "            mask_resized = cv2.resize(mask.astype(np.uint8),\n",
        "                                    (gt_mask.shape[1], gt_mask.shape[0]),\n",
        "                                    interpolation=cv2.INTER_NEAREST)\n",
        "            pred_masks_resized.append(mask_resized)\n",
        "        pred_masks = np.stack(pred_masks_resized, axis=-1)\n",
        "\n",
        "    # Skip if there are no valid ground truth or predictions\n",
        "    if gt_class_ids.size == 0 or r['class_ids'].size == 0:\n",
        "        continue\n",
        "\n",
        "    # Compute AP for the current image\n",
        "    AP, precisions, recalls, overlaps = compute_ap(\n",
        "        gt_bbox,          # Ground truth bounding boxes\n",
        "        gt_class_ids,     # Ground truth class IDs\n",
        "        gt_mask,          # Ground truth masks\n",
        "        r['rois'],        # Predicted bounding boxes\n",
        "        r['class_ids'],   # Predicted class IDs\n",
        "        r['scores'],      # Predicted scores\n",
        "        pred_masks        # Predicted masks\n",
        "    )\n",
        "\n",
        "    APs.append(AP)\n",
        "\n",
        "    # Prepare labels for sklearn metrics\n",
        "    # Convert masks to binary labels for each pixel\n",
        "    gt_mask_binary = np.any(gt_mask > 0, axis=2).astype(np.int32)\n",
        "    pred_mask_binary = np.any(pred_masks > 0, axis=2).astype(np.int32)\n",
        "\n",
        "    # Flatten masks for sklearn metrics\n",
        "    gt_mask_flat = gt_mask_binary.flatten()\n",
        "    pred_mask_flat = pred_mask_binary.flatten()\n",
        "\n",
        "    # Store flattened labels\n",
        "    all_true_labels.extend(gt_mask_flat)\n",
        "    all_pred_labels.extend(pred_mask_flat)\n",
        "\n",
        "# Convert lists to numpy arrays for sklearn metrics\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "all_pred_labels = np.array(all_pred_labels)\n",
        "\n",
        "# Calculate overall metrics\n",
        "if len(APs) > 0:\n",
        "    # Calculate mAP\n",
        "    mAP = np.mean(APs)\n",
        "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
        "\n",
        "    # Calculate sklearn metrics\n",
        "    precision = precision_score(all_true_labels, all_pred_labels, average='binary')\n",
        "    recall = recall_score(all_true_labels, all_pred_labels, average='binary')\n",
        "    f1 = f1_score(all_true_labels, all_pred_labels, average='binary')\n",
        "\n",
        "    # Print all metrics\n",
        "    print(\"\\nOverall Metrics:\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Calculate per-image averages\n",
        "    print(\"\\nPer-image AP Statistics:\")\n",
        "    print(f\"Min AP: {np.min(APs):.4f}\")\n",
        "    print(f\"Max AP: {np.max(APs):.4f}\")\n",
        "    print(f\"Median AP: {np.median(APs):.4f}\")\n",
        "    print(f\"Standard Deviation AP: {np.std(APs):.4f}\")\n",
        "\n",
        "    # Print detailed distribution of APs\n",
        "    percentiles = [25, 50, 75, 90, 95]\n",
        "    print(\"\\nAP Percentiles:\")\n",
        "    for p in percentiles:\n",
        "        print(f\"{p}th percentile: {np.percentile(APs, p):.4f}\")\n",
        "else:\n",
        "    print(\"No valid predictions found for evaluation\")\n",
        "\n",
        "# Save metrics to a file\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "metrics = {\n",
        "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"mAP\": float(mAP),\n",
        "    \"precision\": float(precision),\n",
        "    \"recall\": float(recall),\n",
        "    \"f1_score\": float(f1),\n",
        "    \"num_images_evaluated\": len(APs),\n",
        "    \"ap_statistics\": {\n",
        "        \"min\": float(np.min(APs)),\n",
        "        \"max\": float(np.max(APs)),\n",
        "        \"median\": float(np.median(APs)),\n",
        "        \"std\": float(np.std(APs)),\n",
        "        \"percentiles\": {\n",
        "            str(p): float(np.percentile(APs, p)) for p in percentiles\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save metrics to JSON file\n",
        "with open('evaluation_metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=4)\n",
        "print(\"\\nMetrics saved to 'evaluation_metrics.json'\")\n"
      ],
      "metadata": {
        "id": "01m8q6s9VP5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "namQ-qMbVUDQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}