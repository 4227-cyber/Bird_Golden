{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyMM62YvtkpH9ukVhj8bJCWy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4227-cyber/Bird_Golden/blob/main/Bird.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XMBfuyE9RT0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32bd6719-1004-4e3a-e66b-838e3632b44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Mask-RCNN_TF2.14.0' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/z-mahmud22/Mask-RCNN_TF2.14.0.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Mask-RCNN_TF2.14.0\n"
      ],
      "metadata": {
        "id": "ViWrJv0MRZyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f78b981-b359-4aa3-ec1f-e7b5355d994d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask-RCNN_TF2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)\n"
      ],
      "metadata": {
        "id": "sRIjyomGRevH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a4d9a2-9390-4f29-b28d-076611e2092a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o requirements.txt https://raw.githubusercontent.com/z-mahmud22/Mask-RCNN_TF2.14.0/main/requirements.txt\n"
      ],
      "metadata": {
        "id": "J1FK8hrkRhw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9296ac20-cea1-46af-c736-244655f04879"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   299  100   299    0     0   1791      0 --:--:-- --:--:-- --:--:--  1801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "PM4wuQ4lRj9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fc374879-df86-40ac-eb11-b8a502d661ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cython==3.0.5 (from -r requirements.txt (line 1))\n",
            "  Downloading Cython-3.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting h5py==3.9.0 (from -r requirements.txt (line 2))\n",
            "  Downloading h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: imgaug==0.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (7.34.0)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: ipython-sql==0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.5.0)\n",
            "Collecting keras==2.14.0 (from -r requirements.txt (line 7))\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting matplotlib==3.7.1 (from -r requirements.txt (line 8))\n",
            "  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting numpy==1.23.5 (from -r requirements.txt (line 9))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting opencv-contrib-python==4.8.0.76 (from -r requirements.txt (line 10))\n",
            "  Downloading opencv_contrib_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting opencv-python==4.8.0.76 (from -r requirements.txt (line 11))\n",
            "  Downloading opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting pillow==9.4.0 (from -r requirements.txt (line 12))\n",
            "  Downloading Pillow-9.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting scikit-image==0.19.3 (from -r requirements.txt (line 13))\n",
            "  Downloading scikit_image-0.19.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Collecting scipy==1.11.3 (from -r requirements.txt (line 14))\n",
            "  Downloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard==2.14.1 (from -r requirements.txt (line 15))\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow==2.14.0 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug==0.4.0->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug==0.4.0->-r requirements.txt (line 3)) (2.36.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug==0.4.0->-r requirements.txt (line 3)) (2.0.6)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython==7.34.0->-r requirements.txt (line 4))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->-r requirements.txt (line 4)) (4.9.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from ipython-sql==0.5.0->-r requirements.txt (line 6)) (3.12.0)\n",
            "Requirement already satisfied: sqlalchemy>=2.0 in /usr/local/lib/python3.10/dist-packages (from ipython-sql==0.5.0->-r requirements.txt (line 6)) (2.0.36)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.10/dist-packages (from ipython-sql==0.5.0->-r requirements.txt (line 6)) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1->-r requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 13)) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 13)) (2024.9.20)\n",
            "Collecting PyWavelets>=1.1.1 (from scikit-image==0.19.3->-r requirements.txt (line 13))\n",
            "  Downloading pywavelets-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (1.68.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard==2.14.1->-r requirements.txt (line 15))\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (3.7)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.14.1->-r requirements.txt (line 15)) (3.1.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (18.1.1)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (3.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (0.37.1)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.8.89 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cublas-cu11==11.11.3.6 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.7.0.84 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-curand-cu11==10.3.0.86 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_curand_cu11-10.3.0.86-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.1.48 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.5.86 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-nccl-cu11==2.16.5 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_nccl_cu11-2.16.5-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.8.87 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvcc-cu11==11.8.89 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading nvidia_cuda_nvcc_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting tensorrt==8.5.3.1 (from tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16))\n",
            "  Downloading tensorrt-8.5.3.1-cp310-none-manylinux_2_17_x86_64.whl.metadata (721 bytes)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0->tensorflow[and-cuda]==2.14.0->-r requirements.txt (line 16)) (0.45.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.14.1->-r requirements.txt (line 15)) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.14.1->-r requirements.txt (line 15)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.14.1->-r requirements.txt (line 15)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard==2.14.1->-r requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==7.34.0->-r requirements.txt (line 4)) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==7.34.0->-r requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->-r requirements.txt (line 4)) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.1->-r requirements.txt (line 15)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.1->-r requirements.txt (line 15)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.1->-r requirements.txt (line 15)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.14.1->-r requirements.txt (line 15)) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0->ipython-sql==0.5.0->-r requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.14.1->-r requirements.txt (line 15)) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.14.1->-r requirements.txt (line 15)) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard==2.14.1->-r requirements.txt (line 15)) (3.2.2)\n",
            "Downloading Cython-3.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Pillow-9.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_image-0.19.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl (417.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvcc_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl (875 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.3.0.86-py3-none-manylinux2014_x86_64.whl (58.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux2014_x86_64.whl (128.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux2014_x86_64.whl (204.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.16.5-py3-none-manylinux1_x86_64.whl (210.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.3/210.3 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorrt-8.5.3.1-cp310-none-manylinux_2_17_x86_64.whl (549.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.5/549.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, pillow, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvcc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, keras, jedi, cython, scipy, PyWavelets, opencv-python, opencv-contrib-python, nvidia-cusolver-cu11, nvidia-cudnn-cu11, ml-dtypes, h5py, tensorrt, scikit-image, matplotlib, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.0\n",
            "    Uninstalling wrapt-1.17.0:\n",
            "      Successfully uninstalled wrapt-1.17.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.0.0\n",
            "    Uninstalling pillow-11.0.0:\n",
            "      Successfully uninstalled pillow-11.0.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.11\n",
            "    Uninstalling Cython-3.0.11:\n",
            "      Successfully uninstalled Cython-3.0.11\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.10.0.84\n",
            "    Uninstalling opencv-python-4.10.0.84:\n",
            "      Successfully uninstalled opencv-python-4.10.0.84\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.10.0.84\n",
            "    Uninstalling opencv-contrib-python-4.10.0.84:\n",
            "      Successfully uninstalled opencv-contrib-python-4.10.0.84\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.12.1\n",
            "    Uninstalling h5py-3.12.1:\n",
            "      Successfully uninstalled h5py-3.12.1\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.24.0\n",
            "    Uninstalling scikit-image-0.24.0:\n",
            "      Successfully uninstalled scikit-image-0.24.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.0\n",
            "    Uninstalling matplotlib-3.8.0:\n",
            "      Successfully uninstalled matplotlib-3.8.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.27.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "google-genai 0.1.0 requires pillow<12.0.0,>=10.0.0, but you have pillow 9.4.0 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "plotnine 0.14.3 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n",
            "tensorstore 0.1.69 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.14.0 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyWavelets-1.8.0 cython-3.0.5 google-auth-oauthlib-1.0.0 h5py-3.9.0 jedi-0.19.2 keras-2.14.0 matplotlib-3.7.1 ml-dtypes-0.2.0 numpy-1.23.5 nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvcc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.16.5 opencv-contrib-python-4.8.0.76 opencv-python-4.8.0.76 pillow-9.4.0 scikit-image-0.19.3 scipy-1.11.3 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorrt-8.5.3.1 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              },
              "id": "817ddb563b974fc1a3609707bb9ec905"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 setup.py build\n",
        "\n",
        "!python3 setup.py install\n"
      ],
      "metadata": {
        "id": "fYmbK2GIRlrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6ecb8a-dd5b-4edb-e227-4141d1ab67ad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask-RCNN_TF2.14.0/setup.py:9: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n",
            "WARNING:root:Fail load requirements file, so using default ones.\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'description-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'description_file' instead.\n",
            "\n",
            "        This deprecation is overdue, please update your project and remove deprecated\n",
            "        calls to avoid build errors in the future.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'license-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'license_file' instead.\n",
            "\n",
            "        This deprecation is overdue, please update your project and remove deprecated\n",
            "        calls to avoid build errors in the future.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'requirements-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'requirements_file' instead.\n",
            "\n",
            "        This deprecation is overdue, please update your project and remove deprecated\n",
            "        calls to avoid build errors in the future.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "INFO:root:running build\n",
            "INFO:root:running build_py\n",
            "INFO:root:creating build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/model.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/parallel_model.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/config.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/utils.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/visualize.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/__init__.py -> build/lib/mrcnn\n",
            "INFO:root:running egg_info\n",
            "INFO:root:creating mask_rcnn_tf2.egg-info\n",
            "INFO:root:writing mask_rcnn_tf2.egg-info/PKG-INFO\n",
            "INFO:root:writing dependency_links to mask_rcnn_tf2.egg-info/dependency_links.txt\n",
            "INFO:root:writing top-level names to mask_rcnn_tf2.egg-info/top_level.txt\n",
            "INFO:root:writing manifest file 'mask_rcnn_tf2.egg-info/SOURCES.txt'\n",
            "INFO:root:reading manifest file 'mask_rcnn_tf2.egg-info/SOURCES.txt'\n",
            "INFO:root:reading manifest template 'MANIFEST.in'\n",
            "INFO:root:adding license file 'LICENSE'\n",
            "INFO:root:writing manifest file 'mask_rcnn_tf2.egg-info/SOURCES.txt'\n",
            "/content/Mask-RCNN_TF2.14.0/setup.py:9: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n",
            "WARNING:root:Fail load requirements file, so using default ones.\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'description-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'description_file' instead.\n",
            "\n",
            "        This deprecation is overdue, please update your project and remove deprecated\n",
            "        calls to avoid build errors in the future.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'license-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'license_file' instead.\n",
            "\n",
            "        This deprecation is overdue, please update your project and remove deprecated\n",
            "        calls to avoid build errors in the future.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'requirements-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'requirements_file' instead.\n",
            "\n",
            "        This deprecation is overdue, please update your project and remove deprecated\n",
            "        calls to avoid build errors in the future.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "INFO:root:running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "INFO:root:running bdist_egg\n",
            "INFO:root:running egg_info\n",
            "INFO:root:writing mask_rcnn_tf2.egg-info/PKG-INFO\n",
            "INFO:root:writing dependency_links to mask_rcnn_tf2.egg-info/dependency_links.txt\n",
            "INFO:root:writing top-level names to mask_rcnn_tf2.egg-info/top_level.txt\n",
            "INFO:root:reading manifest file 'mask_rcnn_tf2.egg-info/SOURCES.txt'\n",
            "INFO:root:reading manifest template 'MANIFEST.in'\n",
            "INFO:root:adding license file 'LICENSE'\n",
            "INFO:root:writing manifest file 'mask_rcnn_tf2.egg-info/SOURCES.txt'\n",
            "INFO:root:installing library code to build/bdist.linux-x86_64/egg\n",
            "INFO:root:running install_lib\n",
            "INFO:root:running build_py\n",
            "INFO:root:copying mrcnn/model.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/parallel_model.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/config.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/utils.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/visualize.py -> build/lib/mrcnn\n",
            "INFO:root:copying mrcnn/__init__.py -> build/lib/mrcnn\n",
            "INFO:root:creating build/bdist.linux-x86_64/egg\n",
            "INFO:root:creating build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "INFO:root:byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-310.pyc\n",
            "INFO:root:byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-310.pyc\n",
            "INFO:root:byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-310.pyc\n",
            "INFO:root:byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-310.pyc\n",
            "INFO:root:byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-310.pyc\n",
            "INFO:root:byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-310.pyc\n",
            "INFO:root:creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "INFO:root:copying mask_rcnn_tf2.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "INFO:root:copying mask_rcnn_tf2.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "INFO:root:copying mask_rcnn_tf2.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "INFO:root:copying mask_rcnn_tf2.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "WARNING:root:zip_safe flag not set; analyzing archive contents...\n",
            "INFO:root:creating dist\n",
            "INFO:root:creating 'dist/mask_rcnn_tf2-1.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "INFO:root:removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "INFO:root:Processing mask_rcnn_tf2-1.0-py3.10.egg\n",
            "INFO:root:Copying mask_rcnn_tf2-1.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "INFO:root:Adding mask-rcnn-tf2 1.0 to easy-install.pth file\n",
            "INFO:root:\n",
            "Installed /usr/local/lib/python3.10/dist-packages/mask_rcnn_tf2-1.0-py3.10.egg\n",
            "INFO:root:Processing dependencies for mask-rcnn-tf2==1.0\n",
            "INFO:root:Finished processing dependencies for mask-rcnn-tf2==1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/matterport/Mask_RCNN\n"
      ],
      "metadata": {
        "id": "h98Rx7mnSaOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d8be79-79a4-42a9-ac33-2d7e917f3a72"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-16 10:58:19--  https://github.com/matterport/Mask_RCNN\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘Mask_RCNN’\n",
            "\n",
            "Mask_RCNN               [ <=>                ] 373.06K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-12-16 10:58:20 (33.5 MB/s) - ‘Mask_RCNN’ saved [382010]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "UTXA-sEPScvD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8fde96c-94bc-4a7f-f635-65df3f8e54cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# Specify the path to your dataset (change the path accordingly)\n",
        "data_dir = '/content/drive/MyDrive/Bird'"
      ],
      "metadata": {
        "id": "2OBb_m2zSewf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "from mrcnn.utils import Dataset\n",
        "\n",
        "\n",
        "class BirdDataset(Dataset):\n",
        "    def load_bird(self, dataset_dir, subset):\n",
        "        \"\"\"\n",
        "        Charger un sous-ensemble du dataset Bird.\n",
        "        dataset_dir: Répertoire racine du dataset.\n",
        "        subset: Le sous-ensemble à charger: 'train', 'val', ou 'test'.\n",
        "        \"\"\"\n",
        "        # Ajouter la classe \"golden_pheasant\"\n",
        "        self.add_class(\"bird\", 1, \"golden_pheasant\")\n",
        "\n",
        "        # Définir le chemin du fichier d'annotations pour le sous-ensemble actuel\n",
        "        annotations_path = os.path.join(dataset_dir, subset, \"annotations.coco.json\")\n",
        "        print(annotations_path)\n",
        "\n",
        "        # Charger les annotations COCO\n",
        "        coco = COCO(annotations_path)\n",
        "\n",
        "        # Ajouter les images au dataset\n",
        "        image_ids = coco.getImgIds()\n",
        "        for image_id in image_ids:\n",
        "            image_info = coco.loadImgs(image_id)[0]\n",
        "            image_path = os.path.join(dataset_dir, subset, image_info['file_name'])\n",
        "\n",
        "            # Vérifier si l'image existe avant de l'ajouter\n",
        "            if os.path.exists(image_path):\n",
        "                self.add_image(\n",
        "                    \"bird\",  # Nom de la catégorie\n",
        "                    image_id=image_id,\n",
        "                    path=image_path,\n",
        "                  #  width=image_info['width'],\n",
        "                   # height=image_info['height']\n",
        "                )\n",
        "            else:\n",
        "                print(f\"Image non trouvée : {image_path}\")\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"\n",
        "        Charger les masques d'instances pour une image à partir des boîtes englobantes.\n",
        "        \"\"\"\n",
        "        # Informations de l'image\n",
        "        info = self.image_info[image_id]\n",
        "\n",
        "        # Chemin des annotations\n",
        "        annotations_path = os.path.join(os.path.dirname(info['path']), 'annotations.coco.json')\n",
        "        coco = COCO(annotations_path)\n",
        "\n",
        "        # Charger les annotations pour l'image actuelle\n",
        "        ann_ids = coco.getAnnIds(imgIds=info['id'])\n",
        "        annotations = coco.loadAnns(ann_ids)\n",
        "\n",
        "        masks = []\n",
        "        class_ids = []\n",
        "\n",
        "        # Créer les masques à partir des boîtes englobantes\n",
        "        for ann in annotations:\n",
        "            if 'bbox' in ann:\n",
        "                bbox = ann['bbox']\n",
        "                mask = np.zeros((info['height'], info['width']), dtype=np.uint8)\n",
        "\n",
        "                # Définir les limites de la boîte [x, y, largeur, hauteur]\n",
        "                x, y, width, height = bbox\n",
        "                mask[y:y+ height, x:x+ width] = 1  # Marquer la région de la boîte\n",
        "\n",
        "                masks.append(mask)\n",
        "                class_ids.append(ann['category_id'])  # ID de la catégorie\n",
        "\n",
        "        # Si aucune annotation, retourner un masque vide\n",
        "        if len(masks) == 0:\n",
        "            masks = np.zeros((info['height'], info['width'], 0))\n",
        "            class_ids = np.array([])\n",
        "\n",
        "        # Empiler les masques et convertir les IDs de classe en numpy array\n",
        "        return np.stack(masks, axis=-1), np.array(class_ids)\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"\n",
        "        Retourner le chemin vers l'image dans le dataset.\n",
        "        \"\"\"\n",
        "        return self.image_info[image_id]['path']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k4nX4qTOJ6M5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir les chemins vers les données\n",
        "dataset_dir = '/content/drive/MyDrive/Bird/'  # Répertoire principal contenant les sous-dossiers train et test\n",
        "subset = 'train'\n",
        "# Initialiser les datasets\n",
        "dataset_train = BirdDataset()\n",
        "\n",
        "\n",
        "# Charger les données de l'ensemble d'entraînement\n",
        "dataset_train.load_bird(dataset_dir, \"train\")  # Assurez-vous que \"train\" est un répertoire valide\n",
        "dataset_train.prepare()\n",
        "\n"
      ],
      "metadata": {
        "id": "PDwO3i9QVV6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae314a58-2385-4a99-e525-3b4c8a950fd2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Bird/train/annotations.coco.json\n",
            "loading annotations into memory...\n",
            "Done (t=1.12s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir les chemins vers les données\n",
        "dataset_dir = '/content/drive/MyDrive/Bird/'  # Répertoire principal contenant les sous-dossiers train et test\n",
        "\n",
        "dataset_val = BirdDataset()\n",
        "\n",
        "# Charger les données de l'ensemble de validation\n",
        "dataset_val.load_bird(dataset_dir, \"val\")  # Assurez-vous que \"val\" est un répertoire valide\n",
        "dataset_val.prepare()"
      ],
      "metadata": {
        "id": "dENR5o_gxKdI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ded53c-e41a-4a9a-8ce5-26c49f20b4da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Bird/val/annotations.coco.json\n",
            "loading annotations into memory...\n",
            "Done (t=0.46s)\n",
            "creating index...\n",
            "index created!\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/val/val_annotations.json\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/val/val/DI3YJ9QU6QUO.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chemin vers votre répertoire de données\n",
        "data_dir = '/content/drive/MyDrive/Bird/'\n",
        "subset='test'\n",
        "\n",
        "# Initialiser le dataset de test\n",
        "dataset_test = BirdDataset()  # Remplacez \"BirdDataset\" par le nom de votre classe si nécessaire\n",
        "\n",
        "# Charger le dataset de test\n",
        "dataset_test.load_bird(dataset_dir=data_dir, subset=subset)  # 'test' pour charger les images de test\n",
        "\n",
        "# Préparer le dataset de test (préparer les images et les masques)\n",
        "dataset_test.prepare()\n"
      ],
      "metadata": {
        "id": "OZ3jFibuxZwy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7456766-1c5c-4aba-de2a-ed9c556177be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Bird/test/annotations.coco.json\n",
            "loading annotations into memory...\n",
            "Done (t=0.52s)\n",
            "creating index...\n",
            "index created!\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/0MBJV8Y4NU15.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/US7YLCSIW269.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/HGURDZRX4Z0H.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/YQZ3GKWR3FLP.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/WU1FJ06IWM1N.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/MVZCXDNJLUTP.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/2U1JP99DCDPV.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/B1VI1QJSOT57.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/VC74014XN1UA.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/ZI83TRWEBIA0.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/8SXQOP7QSTWD.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/0MBJV8Y4NU15.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/US7YLCSIW269.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/HGURDZRX4Z0H.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/YQZ3GKWR3FLP.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/WU1FJ06IWM1N.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/MVZCXDNJLUTP.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/2U1JP99DCDPV.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/B1VI1QJSOT57.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/VC74014XN1UA.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/ZI83TRWEBIA0.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/8SXQOP7QSTWD.jpg\n",
            "Image non trouvée : /content/drive/MyDrive/Bird/test/test/TL1U9WJ46Z2E.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.model import MaskRCNN\n",
        "\n",
        "# Define your custom configuration class\n",
        "class CustomConfig(Config):\n",
        "    NAME = \"Bird\"  # Name of the dataset or task\n",
        "    IMAGES_PER_GPU = 2  # Adjust this based on the memory of your GPU\n",
        "    NUM_CLASSES = 2  # Including background (1) + Bird (1)\n",
        "    STEPS_PER_EPOCH = 100  # Set based on the number of images in your dataset\n",
        "    VALIDATION_STEPS = 100  # Set validation steps based on the dataset size\n",
        "    LEARNING_RATE = 0.001  # Learning rate for training\n",
        "    IMAGE_MIN_DIM = 512  # Minimum image dimension for resizing\n",
        "    IMAGE_MAX_DIM = 512  # Maximum image dimension for resizing\n",
        "    GPU_COUNT = 1  # Number of GPUs (adjust if you have more)\n",
        "\n",
        "# Instantiate your custom configuration\n",
        "config = CustomConfig()"
      ],
      "metadata": {
        "id": "ZbJFwnA1U9yi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77V4u-GyGFxH",
        "outputId": "f49a58b9-7e12-4940-c059-d7bbe406144f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-16 11:01:53--  https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241216%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241216T110153Z&X-Amz-Expires=300&X-Amz-Signature=b616add5161d3bb0682383f82e1b478f32b0eb18d05b5c530cd1cc8f37e3e756&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-16 11:01:53--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241216%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241216T110153Z&X-Amz-Expires=300&X-Amz-Signature=b616add5161d3bb0682383f82e1b478f32b0eb18d05b5c530cd1cc8f37e3e756&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257557808 (246M) [application/octet-stream]\n",
            "Saving to: ‘mask_rcnn_coco.h5’\n",
            "\n",
            "mask_rcnn_coco.h5   100%[===================>] 245.63M   146MB/s    in 1.7s    \n",
            "\n",
            "2024-12-16 11:01:55 (146 MB/s) - ‘mask_rcnn_coco.h5’ saved [257557808/257557808]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Create the model in training mode\n",
        "model = MaskRCNN(mode=\"training\", config=config, model_dir='/content')\n",
        "\n",
        "# Step 2: Load pre-trained weights (COCO weights) but exclude the conflicting layers\n",
        "model.load_weights('mask_rcnn_coco.h5', by_name=True, exclude=[\"mrcnn_bbox_fc\", \"mrcnn_class_logits\", \"mrcnn_mask\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "1F186C_AVCZR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(dataset_train, dataset_val, learning_rate=config.LEARNING_RATE, epochs=10, layers='heads')\n"
      ],
      "metadata": {
        "id": "8WapeIVjwlSu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "outputId": "7b886bf4-1121-43d6-90f0-103ce7c180e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting at epoch 0. LR=0.001\n",
            "\n",
            "Checkpoint Path: /content/bird20241216T1102/mask_rcnn_bird_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "rpn_model              (Functional)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-39f3dc803f35>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'heads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mask_rcnn_tf2-1.0-py3.10.egg/mrcnn/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2371\u001b[0;31m         self.keras_model.fit(\n\u001b[0m\u001b[1;32m   2372\u001b[0m             \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m             \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         return func.fit(\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_generator_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         )\n\u001b[0;32m--> 647\u001b[0;31m         return fit_generator(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_generator_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_generator_v1.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/data_utils.py\u001b[0m in \u001b[0;36miter_sequence_infinite\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \"\"\"\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/data_utils.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mask_rcnn_tf2-1.0-py3.10.egg/mrcnn/model.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Generate Confusion Matrix after training\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Step 4.1: Create a model for inference\n",
        "class InferenceConfig(Config):\n",
        "    NAME = \"bird\"\n",
        "    NUM_CLASSES = 1 + 1\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "inference_model = MaskRCNN(mode=\"inference\", config=inference_config, model_dir='/content/drive/Mydrive/Bird/data/train')\n",
        "\n",
        "# Load the trained weights into the inference model\n",
        "inference_model.load_weights(model.find_last(), by_name=True)\n",
        "\n",
        "# Step 4.2: Collect true and predicted labels\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for image_id in dataset_test.image_ids:\n",
        "    # Load true labels (class IDs) from the dataset\n",
        "    true_masks, true_classes = dataset_test.load_mask(image_id)\n",
        "\n",
        "    # Predict the image using the inference model\n",
        "    image = dataset_test.load_image(image_id)\n",
        "    pred = inference_model.detect([image], verbose=0)[0]\n",
        "\n",
        "    # Align true and predicted classes\n",
        "    if len(true_classes) == len(pred['class_ids']):\n",
        "        true_labels.extend(true_classes)\n",
        "        pred_labels.extend(pred['class_ids'])\n",
        "    else:\n",
        "        # Handle mismatched cases\n",
        "        true_labels.extend(true_classes)\n",
        "        pred_labels.extend(pred['class_ids'])\n",
        "\n",
        "        # Fill with '0' (background class) for unmatched entries\n",
        "        if len(true_classes) > len(pred['class_ids']):\n",
        "            pred_labels.extend([0] * (len(true_classes) - len(pred['class_ids'])))\n",
        "        elif len(pred['class_ids']) > len(true_classes):\n",
        "            true_labels.extend([0] * (len(pred['class_ids']) - len(true_classes)))\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 4.3: Generate and plot the confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1])  # Add '0' for background\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['background', 'Bird_Golden'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Step 5: Visualize test predictions (Successes and Mistakes)\n",
        "from mrcnn.visualize import display_instances\n",
        "from mrcnn import visualize\n",
        "\n",
        "import random\n",
        "\n",
        "# Select 5 random image IDs from the test dataset\n",
        "for i in random.sample(list(dataset_test.image_ids), 5):\n",
        "    image = dataset_test.load_image(i)  # Load the image\n",
        "    results = inference_model.detect([image], verbose=0)  # Perform detection\n",
        "\n",
        "    # Visualize the prediction\n",
        "    visualize.display_instances(\n",
        "        image,\n",
        "        results[0]['rois'],\n",
        "        results[0]['masks'],\n",
        "        results[0]['class_ids'],\n",
        "        dataset_test.class_names,\n",
        "        results[0]['scores']\n",
        "    )"
      ],
      "metadata": {
        "id": "oOYnssGXVEbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mrcnn.utils import compute_ap\n",
        "import numpy as np\n",
        "from mrcnn import utils\n",
        "\n",
        "# Store Average Precision (AP) for each image\n",
        "APs = []\n",
        "\n",
        "# Iterate over all test images\n",
        "for image_id in dataset_test.image_ids:\n",
        "    # Load the image\n",
        "    image = dataset_test.load_image(image_id)\n",
        "\n",
        "    # Load ground truth data: masks and class IDs\n",
        "    gt_mask, gt_class_ids = dataset_test.load_mask(image_id)\n",
        "\n",
        "    # Generate ground truth bounding boxes (compute from masks if not pre-defined)\n",
        "    gt_bbox = utils.extract_bboxes(gt_mask)\n",
        "\n",
        "    # Run inference on the image\n",
        "    results = inference_model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "\n",
        "    # Skip images with no predictions\n",
        "    if r['masks'].size == 0:\n",
        "        continue\n",
        "\n",
        "    # Ensure predicted masks are the same size as the ground truth masks\n",
        "    pred_masks = r['masks']\n",
        "\n",
        "    # If the prediction masks are larger than needed, resize them to match ground truth\n",
        "    if pred_masks.shape[0] != gt_mask.shape[0] or pred_masks.shape[1] != gt_mask.shape[1]:\n",
        "        pred_masks_resized = []\n",
        "        for i in range(pred_masks.shape[-1]):\n",
        "            mask = pred_masks[:, :, i]\n",
        "            mask_resized = cv2.resize(mask.astype(np.uint8),\n",
        "                                    (gt_mask.shape[1], gt_mask.shape[0]),\n",
        "                                    interpolation=cv2.INTER_NEAREST)\n",
        "            pred_masks_resized.append(mask_resized)\n",
        "        pred_masks = np.stack(pred_masks_resized, axis=-1)\n",
        "\n",
        "    # Skip if there are no valid ground truth or predictions\n",
        "    if gt_class_ids.size == 0 or r['class_ids'].size == 0:\n",
        "        continue\n",
        "\n",
        "    # Compute AP for the current image\n",
        "    AP, precisions, recalls, overlaps = compute_ap(\n",
        "        gt_bbox,          # Ground truth bounding boxes\n",
        "        gt_class_ids,     # Ground truth class IDs\n",
        "        gt_mask,          # Ground truth masks\n",
        "        r['rois'],        # Predicted bounding boxes\n",
        "        r['class_ids'],   # Predicted class IDs\n",
        "        r['scores'],      # Predicted scores (was missing in original)\n",
        "        pred_masks        # Predicted masks\n",
        "    )\n",
        "\n",
        "    APs.append(AP)\n",
        "\n",
        "# Compute and display the mean Average Precision (mAP)\n",
        "if len(APs) > 0:\n",
        "    mAP = np.mean(APs)\n",
        "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
        "else:\n",
        "    print(\"No valid predictions found for evaluation\")"
      ],
      "metadata": {
        "id": "lf3wOQQDVPhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mrcnn.utils import compute_ap\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import cv2\n",
        "\n",
        "# Store metrics for each image\n",
        "APs = []\n",
        "all_true_labels = []\n",
        "all_pred_labels = []\n",
        "\n",
        "# Iterate over all test images\n",
        "for image_id in dataset_test.image_ids:\n",
        "    # Load the image\n",
        "    image = dataset_test.load_image(image_id)\n",
        "\n",
        "    # Load ground truth data: masks and class IDs\n",
        "    gt_mask, gt_class_ids = dataset_test.load_mask(image_id)\n",
        "\n",
        "    # Generate ground truth bounding boxes\n",
        "    gt_bbox = utils.extract_bboxes(gt_mask)\n",
        "\n",
        "    # Run inference on the image\n",
        "    results = inference_model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "\n",
        "    # Skip images with no predictions\n",
        "    if r['masks'].size == 0:\n",
        "        continue\n",
        "\n",
        "    # Ensure predicted masks are the same size as the ground truth masks\n",
        "    pred_masks = r['masks']\n",
        "\n",
        "    # Resize prediction masks if needed\n",
        "    if pred_masks.shape[0] != gt_mask.shape[0] or pred_masks.shape[1] != gt_mask.shape[1]:\n",
        "        pred_masks_resized = []\n",
        "        for i in range(pred_masks.shape[-1]):\n",
        "            mask = pred_masks[:, :, i]\n",
        "            mask_resized = cv2.resize(mask.astype(np.uint8),\n",
        "                                    (gt_mask.shape[1], gt_mask.shape[0]),\n",
        "                                    interpolation=cv2.INTER_NEAREST)\n",
        "            pred_masks_resized.append(mask_resized)\n",
        "        pred_masks = np.stack(pred_masks_resized, axis=-1)\n",
        "\n",
        "    # Skip if there are no valid ground truth or predictions\n",
        "    if gt_class_ids.size == 0 or r['class_ids'].size == 0:\n",
        "        continue\n",
        "\n",
        "    # Compute AP for the current image\n",
        "    AP, precisions, recalls, overlaps = compute_ap(\n",
        "        gt_bbox,          # Ground truth bounding boxes\n",
        "        gt_class_ids,     # Ground truth class IDs\n",
        "        gt_mask,          # Ground truth masks\n",
        "        r['rois'],        # Predicted bounding boxes\n",
        "        r['class_ids'],   # Predicted class IDs\n",
        "        r['scores'],      # Predicted scores\n",
        "        pred_masks        # Predicted masks\n",
        "    )\n",
        "\n",
        "    APs.append(AP)\n",
        "\n",
        "    # Prepare labels for sklearn metrics\n",
        "    # Convert masks to binary labels for each pixel\n",
        "    gt_mask_binary = np.any(gt_mask > 0, axis=2).astype(np.int32)\n",
        "    pred_mask_binary = np.any(pred_masks > 0, axis=2).astype(np.int32)\n",
        "\n",
        "    # Flatten masks for sklearn metrics\n",
        "    gt_mask_flat = gt_mask_binary.flatten()\n",
        "    pred_mask_flat = pred_mask_binary.flatten()\n",
        "\n",
        "    # Store flattened labels\n",
        "    all_true_labels.extend(gt_mask_flat)\n",
        "    all_pred_labels.extend(pred_mask_flat)\n",
        "\n",
        "# Convert lists to numpy arrays for sklearn metrics\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "all_pred_labels = np.array(all_pred_labels)\n",
        "\n",
        "# Calculate overall metrics\n",
        "if len(APs) > 0:\n",
        "    # Calculate mAP\n",
        "    mAP = np.mean(APs)\n",
        "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
        "\n",
        "    # Calculate sklearn metrics\n",
        "    precision = precision_score(all_true_labels, all_pred_labels, average='binary')\n",
        "    recall = recall_score(all_true_labels, all_pred_labels, average='binary')\n",
        "    f1 = f1_score(all_true_labels, all_pred_labels, average='binary')\n",
        "\n",
        "    # Print all metrics\n",
        "    print(\"\\nOverall Metrics:\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Calculate per-image averages\n",
        "    print(\"\\nPer-image AP Statistics:\")\n",
        "    print(f\"Min AP: {np.min(APs):.4f}\")\n",
        "    print(f\"Max AP: {np.max(APs):.4f}\")\n",
        "    print(f\"Median AP: {np.median(APs):.4f}\")\n",
        "    print(f\"Standard Deviation AP: {np.std(APs):.4f}\")\n",
        "\n",
        "    # Print detailed distribution of APs\n",
        "    percentiles = [25, 50, 75, 90, 95]\n",
        "    print(\"\\nAP Percentiles:\")\n",
        "    for p in percentiles:\n",
        "        print(f\"{p}th percentile: {np.percentile(APs, p):.4f}\")\n",
        "else:\n",
        "    print(\"No valid predictions found for evaluation\")\n",
        "\n",
        "# Save metrics to a file\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "metrics = {\n",
        "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"mAP\": float(mAP),\n",
        "    \"precision\": float(precision),\n",
        "    \"recall\": float(recall),\n",
        "    \"f1_score\": float(f1),\n",
        "    \"num_images_evaluated\": len(APs),\n",
        "    \"ap_statistics\": {\n",
        "        \"min\": float(np.min(APs)),\n",
        "        \"max\": float(np.max(APs)),\n",
        "        \"median\": float(np.median(APs)),\n",
        "        \"std\": float(np.std(APs)),\n",
        "        \"percentiles\": {\n",
        "            str(p): float(np.percentile(APs, p)) for p in percentiles\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save metrics to JSON file\n",
        "with open('evaluation_metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=4)\n",
        "print(\"\\nMetrics saved to 'evaluation_metrics.json'\")\n"
      ],
      "metadata": {
        "id": "01m8q6s9VP5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "namQ-qMbVUDQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}